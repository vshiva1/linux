/*
 * Intel Cache Quality-of-Service Monitoring (CQM) support.
 *
 * A Resource Manager ID (RMID) is a u32 value that, when programmed in a
 * logical CPU, will allow the LLC cache to associate the changes in occupancy
 * generated by that cpu (cache lines allocations - deallocations) to the RMID.
 * If a RMID has been assigned to a thread T long enough for all cache lines
 * used by T to be allocated, then the occupancy reported by the hardware
 * equals the total cache occupancy for T.
 *
 * Groups of threads that are to be monitored together (such as cgroups
 * or processes) can shared a RMID.
 *
 * This driver implements a tree hierarchy of Monitored Resources (monr). Each
 * monr is a cgroup, a process or a thread that needs one single RMID.
 *
 * Since the number of RMIDs is relatively small to the number of potential
 * monitored elements, RMIDs must be "rotated" among all monitored elements.
 */

#include <linux/perf_event.h>
#include <asm/pqr_common.h>
#include <asm/topology.h>

struct sample {
	u64			interval_bytes;
	u64			prev_msr;
};

/*
 * struct prmid: Package RMID. Per-package wrapper for a rmid.
 * @last_read_value:	Least read value.
 * @last_read_time:	Time last read, used when throtling read rate.
 * @pool_entry:		Attaches to a prmid pool in cqm_pkg_data.
 * @rmid:		The rmid value to be programed in hardware.
 *
 * Its accesors ensure that CQM events for this rmid are read atomically and
 * allow to throtle the frequency of reads to up to one each
 * __rmid_min_update_time ms.
 */
struct prmid {
	atomic64_t		last_read_value;
	atomic64_t		last_read_time;
	struct sample		tb;
	struct sample		lb;
	struct list_head	pool_entry;
	u32			rmid;
};

/*
 * Minimum time elapsed between reads of occupancy value for an RMID when
 * transversing the monr hierarchy.
 */
#define RMID_DEFAULT_MIN_UPDATE_TIME 20	/* ms */
static unsigned int __rmid_min_update_time = RMID_DEFAULT_MIN_UPDATE_TIME;

static inline int cqm_prmid_update(struct prmid *prmid, unsigned long *ebm);

#define RMID_DEFAULT_TIMED_UPDATE_PERIOD 100 /* ms */
static unsigned int __rmid_timed_update_period =
	RMID_DEFAULT_TIMED_UPDATE_PERIOD;

/*
 * union prmid_summary: Machine-size summary of a pmonr's prmid state.
 * @value:		One word accesor.
 * @rmid:		rmid for prmid.
 * @sched_rmid:		The rmid to write in the PQR MSR.
 * @read_rmid:		The rmid to read occupancy from.
 *
 * The prmid_summarys are read atomically and without the need of LOCK
 * instructions during event and group scheduling in task context switch.
 * They are set when a prmid change state and allow lock-free fast paths for
 * RMID scheduling and RMID read for the common case when prmid does not need
 * to change state.
 * The combination of values in sched_rmid and read_rmid indicate the state of
 * the associated pmonr (see pmonr comments) as follows:
 *					pmonr state
 *	      |	 (A)state	  (IN)state	   (IL)state	    (U)state
 * ----------------------------------------------------------------------------
 * sched_rmid |	pmonr.prmid	ancestor.prmid	ancestor.prmid	   INVALID_RMID
 *  read_rmid |	pmonr.prmid	INVALID_RMID	pmonr.limbo_prmid  INVALID_RMID
 *								      (or 0)
 *
 * The combination sched_rmid == INVALID_RMID and read_rmid == 0 for (U)state
 * denotes that the flag MONR_MON_ACTIVE is set in the monr associated with
 * the pmonr for this prmid_summary.
 */
union prmid_summary {
	long long	value;
	struct {
		u32	sched_rmid;
		u32	read_rmid;
	};
};

/* A pmonr in (U)state has no sched_rmid, read_rmid can be 0 or INVALID_RMID
 * depending on whether monitoring is active or not.
 */
inline bool prmid_summary__is_ustate(union prmid_summary summ)
{
	return summ.sched_rmid == INVALID_RMID;
}

/* A pmonr in (I)state (either (IN)state or (IL)state. */
inline bool prmid_summary__is_istate(union prmid_summary summ)
{
	return summ.sched_rmid != INVALID_RMID &&
	       summ.sched_rmid != summ.read_rmid;
}

inline bool prmid_summary__is_mon_active(union prmid_summary summ)
{
	/* If not in (U)state, then MONR_MON_ACTIVE must be set. */
	return summ.sched_rmid != INVALID_RMID ||
	       summ.read_rmid == 0;
}

struct monr;

/* struct pmonr: Node of per-package hierarchy of MONitored Resources.
 * @ancestor_pmonr:		lowest active pmonr whose monr is ancestor of
 *				this pmonr's monr.
 * @pmonr_deps_head:		List of pmonrs without prmid that use
 *				this pmonr's prmid -when in (A)state-.
 * @prmid:			The prmid of this pmonr -when in (A)state-.
 * @pmonr_deps_entry:		Entry into ancestor's @pmonr_deps_head
 *				-when inheriting, (I)state-.
 * @limbo_prmid:		A prmid previously used by this pmonr and that
 *				has not been reused yet and therefore contain
 *				occupancy that should be counted towards this
 *				pmonr's occupancy.
 *				The limbo_prmid can be reused in the same pmonr
 *				in the next transition to (A) state, even if
 *				the occupancy of @limbo_prmid is not below the
 *				dirty threshold, reducing the need of free
 *				prmids.
 * @limbo_rotation_entry:	List entry to attach to ilstate_pmonrs_lru when
 *				this pmonr is in (IL)state.
 * @last_enter_istate:		Time last enter (I)state.
 * @last_enter_astate:		Time last enter (A)state. Used in rotation logic
 *				to guarantee that each pmonr gets a minimum
 *				time in	(A)state.
 * @rotation_entry:		List entry to attach to pmonr rotation lists in
 *				pkg_data.
 * @monr:			The monr that contains this pmonr.
 * @nr_enter_istate:		Track number of times entered (I)state. Useful
 *				signal to diagnose excessive contention for
 *				rmids in this package.
 * @pkg_id:			Auxiliar variable with pkg id for this pmonr.
 * @prmid_summary_atomic:	Atomic accesor to store a union prmid_summary
 *				that represent the state of this pmonr.
 *
 * A pmonr forms a per-package hierarchy of prmids. Each one represents a
 * resource to be monitored and can hold a prmid. Due to rmid scarcity,
 * rmids can be recycled and rotated. When a rmid is not available for this
 * pmonr, the pmonr utilizes the rmid of its ancestor.
 * A pmonr is always in one of the following states:
 *   - (A)ctive:	Has @prmid assigned, @ancestor_pmonr must be NULL.
 *   - (I)nherited:	The prmid used is "Inherited" from @ancestor_pmonr.
 *			@ancestor_pmonr must be set. @prmid is unused. This is
 *			a super-state composed of two substates:
 *
 *     - (IL)state:	A pmonr in (I)state that has a valid limbo_prmid.
 *     - (IN)state:	A pmonr in (I)state with NO valid limbo_prmid.
 *
 *			When the distintion between the two substates is
 *			no relevant, the pmonr is simply in the (I)state.
 *   - (U)nused:	No @ancestor_pmonr and no @prmid, hence no available
 *			prmid and no inhering one either. Not in rotation list.
 *			This state is unschedulable and a prmid
 *			should be found (either o free one or ancestor's) before
 *			scheduling a thread with (U)state pmonr in
 *			a cpu in this package.
 *
 * The state transitions are:
 *   (U) : The initial state. Starts there after allocation.
 *   (U) -> (A): If on first sched (or initialization) pmonr receives a prmid.
 *   (U) -> (I): If on first sched (or initialization) pmonr cannot find a free
 *               prmid and resort to use its ancestor's.
 *   (A) -> (I): On stealing of prmid from pmonr (by rotation logic only).
 *   (A) -> (U): On destruction of monr.
 *   (I) -> (A): On receiving a free prmid or on reuse of its @limbo_prmid (by
 *		 rotation logic only).
 *   (I) -> (U): On destruction of pmonr.
 *
 * Note that the (I) -> (A) transition makes monitoring available, but can
 * introduce error due to cache lines allocated before the transition. Such
 * error is likely to decrease over time.
 * When entering an (I) state, the reported count of the event is unavaiable.
 *
 * Each pmonr is contained by a monr. Each monr forms a system-wide hierarchy
 * that is used by the pmrs to find ancestors and dependants. The per-package
 * hierarchy spanned by the pmrs follows the monr hierarchy except by
 * collapsing the nodes in (I)state into a super-node that contains an (A)state
 * pmonr and all of its dependants (pmonr in pmonr_deps_head).
 */
struct pmonr {

	/* If set, pmonr is in (I)state. */
	struct pmonr				*ancestor_pmonr;
	unsigned long				evtfirst_bm;
	struct sample				tb;
	struct sample				lb;

	union{
		struct { /* (A)state variables. */
			struct list_head	pmonr_deps_head;
			struct prmid		*prmid;
		};
		struct { /* (I)state variables. */
			struct list_head	pmonr_deps_entry;
			struct prmid		*limbo_prmid;
			struct list_head	limbo_rotation_entry;
		};
	};

	struct monr				*monr;
	struct list_head			rotation_entry;

	unsigned long				last_enter_istate;
	unsigned long				last_enter_astate;
	unsigned int				nr_enter_istate;

	u16					pkg_id;

	/* all writers are sync'ed by package's lock. */
	atomic64_t				prmid_summary_atomic;
};

/* Store all RMIDs that can fit in a anode while keeping sizeof(struct anode)
 * within one cache line (for performance).
 */
#define NR_TYPE_PER_NODE(__type) ((SMP_CACHE_BYTES - (int)sizeof(struct list_head)) / \
	(int)sizeof(__type))

typedef struct anode_evt_info {
	u32 rmid;
	unsigned long evt_bm;
}anode_etype;

#define NR_RMIDS_PER_NODE NR_TYPE_PER_NODE(anode_etype)

/* struct anode: Node of an array list used to temporarily store RMIDs. */
struct anode {
	/* Last valid RMID is RMID_INVALID */
	struct anode_evt_info	einfo[NR_RMIDS_PER_NODE];
	struct list_head	entry;
};

/*
 * struct pkg_data: Per-package CQM data.
 * @max_rmid:			Max rmid valid for cpus in this package.
 * @prmids_by_rmid:		Utility mapping between rmid values and prmids.
 *				XXX: Make it an array of prmids.
 * @free_prmid_pool:		Free prmids.
 * @active_prmid_pool:		prmids associated with a (A)state pmonr.
 * @pmonr_limbo_prmid_pool:	limbo prmids referenced by the limbo_prmid of a
 *				pmonr in (I)state.
 * @nopmonr_limbo_prmid_pool:	prmids in limbo state that are not referenced
 *				by a pmonr.
 * @astate_pmonrs_lru:		pmonrs in (A)state. LRU in increasing order of
 *				pmonr.last_enter_astate.
 * @istate_pmonrs_lru:		pmors In (I)state with no limbo_prmid. LRU in
 *				increasing order of pmonr.last_enter_istate.
 * @ilsate_pmonrs_lru:		pmonrs in (IL)state, these pmonrs have a valid
 *				limbo_prmid. It's a subset of istate_pmonrs_lru.
 *				Sorted increasingly by pmonr.last_enter_istate.
 * @nr_instate_pmonrs		nr of pmonrs in (IN)state.
 * @nr_ilstate_pmonrs		nr of pmonrs in (IL)state.
 * @pkg_data_mutex:		Hold for stability when modifying pmonrs
 *				hierarchy.
 * @pkg_data_lock:		Hold to protect variables that may be accessed
 *				during process scheduling. The locks for all
 *				packages must be held when modifying the monr
 *				hierarchy.
 * @rotation_work:		Task that performs rotation of prmids.
 * @rotation_cpu:               CPU to run @rotation_work on, it must be in the
 *                              package associated to this instance of pkg_data.
 * @timed_update_work:		Task that performs periodic updates of values
 *				for active rmids. These values are used when
 *				inter-package event read is not available due to
 *				irqs disabled contexts.
 * @timed_update_cpu:		CPU to run @timed_update_work on, it must be a
 *				cpu in this package.
 * @anode_pool_head:		Pool of unused anodes.
 * @anode_pool_lock:		Protect @anode_pool_head.
 */
struct pkg_data {
	u32			max_rmid;
	/* Quick map from rmids to prmids. */
	struct prmid		**prmids_by_rmid;

	/*
	 * Pools of prmids used in rotation logic.
	 */
	struct list_head	free_prmids_pool;
	/* Can be modified during task switch with (U)state -> (A)state. */
	struct list_head	active_prmids_pool;
	/* Only modified during rotation logic and deletion. */
	struct list_head	pmonr_limbo_prmids_pool;
	struct list_head	nopmonr_limbo_prmids_pool;

	struct list_head	astate_pmonrs_lru;
	/* Superset of ilstate_pmonrs_lru. */
	struct list_head	istate_pmonrs_lru;
	struct list_head	ilstate_pmonrs_lru;

	int			nr_instate_pmonrs;
	int			nr_ilstate_pmonrs;

	struct mutex		pkg_data_mutex;
	raw_spinlock_t		pkg_data_lock;

	struct delayed_work	rotation_work;
	int			rotation_cpu;

	struct delayed_work	timed_update_work;
	int			timed_update_cpu;

	/* Pool of unused rmid_list_nodes and its lock */
	struct list_head	anode_pool_head;
	raw_spinlock_t		anode_pool_lock;
};

/*
 * Flags for monr.
 */
#define MONR_MON_ACTIVE		0x1

/*
 * struct monr: MONitored Resource.
 * @flags:		Flags field for monr (XXX: More flags will be added
 *			with MBM).
 * @mon_cgrp:		The cgroup associated with this monr, if any
 * @mon_event_group:	The head of event's group that use this monr, if any.
 * @parent:		Parent in monr hierarchy.
 * @children:		List of children in monr hierarchy.
 * @parent_entry:	Entry in parent's children list.
 * @pmonrs:		Per-package pmonr for this monr.
 *
 * Each cgroup or thread that requires a RMID will have a corresponding
 * monr in the system-wide hierarchy reflecting it's position in the
 * cgroup/thread hierarchy.
 * An monr is assigned to every CQM event and/or monitored cgroups when
 * monitoring is activated and that instance's address do not change during
 * the lifetime of the event or cgroup.
 *
 * On creation, the monr has flags cleared and all its pmonrs in (U)state.
 * The flag MONR_MON_ACTIVE must be set to enable any transition out of
 * (U)state to occur.
 */
struct monr {
	u16				flags;

	unsigned long			evtinfo_bm;
	int				cqm_evt_count;
	/* mbm specific data */
	atomic64_t			total_bytes;
	atomic64_t			local_bytes;
	int				tbyte_evt_count;
	int				lbyte_evt_count;

	/* Back reference pointers */
	struct perf_cgroup		*mon_cgrp;
	struct perf_event		*mon_event_group;

	struct monr			*parent;
	struct list_head		children;
	struct list_head		parent_entry;
	struct pmonr			**pmonrs;
};

/*
 * Root for system-wide hierarchy of monr.
 * A per-package raw_spin_lock protects changes to the per-pkg elements of
 * the monr hierarchy.
 * To modify the monr hierarchy, must hold all locks in each package
 * using packaged-id as nesting parameter.
 */
extern struct monr *monr_hrchy_root;

extern struct pkg_data **cqm_pkgs_data;

static inline u16 __cqm_pkgs_data_next_online(u16 pkg_id)
{
	while (!cqm_pkgs_data[++pkg_id] && pkg_id < topology_max_packages())
		;
	return pkg_id;
}

static inline u16 __cqm_pkgs_data_first_online(void)
{
	if (cqm_pkgs_data[0])
		return 0;
	return __cqm_pkgs_data_next_online(0);
}

/* Iterate for each online pkgs data */
#define cqm_pkg_id_for_each_online(pkg_id__) \
	for (pkg_id__ = __cqm_pkgs_data_first_online(); \
	     pkg_id__ < topology_max_packages(); \
	     pkg_id__ = __cqm_pkgs_data_next_online(pkg_id__))

#define __pkg_data(pmonr, member) cqm_pkgs_data[pmonr->pkg_id]->member

/*
 * Utility function and macros to manage per-package locks.
 * Use macros to keep flags in caller's stace.
 * Hold lock in all the packages, required to alter the monr hierarchy
 */
static inline void monr_hrchy_acquire_mutexes(void)
{
	int i;

	cqm_pkg_id_for_each_online(i)
		mutex_lock_nested(&cqm_pkgs_data[i]->pkg_data_mutex, i);
}

# define monr_hrchy_acquire_raw_spin_locks_irq_save(flags, i) \
	do { \
		raw_local_irq_save(flags); \
		cqm_pkg_id_for_each_online(i) {\
			raw_spin_lock_nested( \
				&cqm_pkgs_data[i]->pkg_data_lock, i); \
		} \
	} while (0)

#define monr_hrchy_acquire_locks(flags, i) \
	do {\
		monr_hrchy_acquire_mutexes(); \
		monr_hrchy_acquire_raw_spin_locks_irq_save(flags, i); \
	} while (0)

static inline void monr_hrchy_release_mutexes(void)
{
	int i;

	cqm_pkg_id_for_each_online(i)
		mutex_unlock(&cqm_pkgs_data[i]->pkg_data_mutex);
}

# define monr_hrchy_release_raw_spin_locks_irq_restore(flags, i) \
	do { \
		cqm_pkg_id_for_each_online(i) {\
			raw_spin_unlock(&cqm_pkgs_data[i]->pkg_data_lock); \
		} \
		raw_local_irq_restore(flags); \
	} while (0)

#define monr_hrchy_release_locks(flags, i) \
	do {\
		monr_hrchy_release_raw_spin_locks_irq_restore(flags, i); \
		monr_hrchy_release_mutexes(); \
	} while (0)

static inline void monr_hrchy_assert_held_mutexes(void)
{
	int i;

	cqm_pkg_id_for_each_online(i)
		lockdep_assert_held(&cqm_pkgs_data[i]->pkg_data_mutex);
}

static inline void monr_hrchy_assert_held_raw_spin_locks(void)
{
	int i;

	cqm_pkg_id_for_each_online(i)
		lockdep_assert_held(&cqm_pkgs_data[i]->pkg_data_lock);
}
#ifdef CONFIG_LOCKDEP
static inline int monr_hrchy_count_held_raw_spin_locks(void)
{
	int i, nr_held = 0;

	cqm_pkg_id_for_each_online(i) {
		if (lockdep_is_held(&cqm_pkgs_data[i]->pkg_data_lock))
			nr_held++;
	}
	return nr_held;
}
#endif

/*
 * Time between execution of rotation logic. The frequency of execution does
 * not affect the rate at which RMIDs are recycled, except by the delay by the
 * delay updating the prmid's and their pools.
 * The rotation period is stored in pmu->hrtimer_interval_ms.
 */
#define CQM_DEFAULT_ROTATION_PERIOD 1200	/* ms */

/*
 * Rotation function.
 * Rotation logic runs per-package. In each package, if free rmids are needed,
 * it will steal prmids from the pmonr that has been the longest time in
 * (A)state.
 * The hardware provides to way to signal that a rmid will be reused, therefore,
 * before reusing a rmid that has been stolen, the rmid should stay for some
 * in a "limbo" state where is not associated to any thread, hoping that the
 * cache lines allocated for this rmid will eventually be replaced.
 */
static void intel_cqm_rmid_rotation_work(struct work_struct *work);

static void intel_cqm_timed_update_work(struct work_struct *work);

/*
 * Service Level Objectives (SLO) for the rotation logic.
 *
 * @__cqm_min_duration_mon_slice: Minimum duration of a monitored slice.
 * @__cqm_max_wait_monitor: Maximum time that a pmonr can pass waiting for an
 *   RMID without rotation logic making any progress. Once elapsed for any
 *  prmid, the reusing threshold (__intel_cqm_max_threshold) can be increased,
 *  potentially increasing the speed at which RMIDs are reused, but potentially
 *  introducing measurement error.
 */
#define CQM_DEFAULT_MIN_MON_SLICE 2000	/* ms */
static unsigned int __cqm_min_mon_slice = CQM_DEFAULT_MIN_MON_SLICE;

#define CQM_DEFAULT_MAX_WAIT_MON 20000 /* ms */
static unsigned int __cqm_max_wait_mon = CQM_DEFAULT_MAX_WAIT_MON;

/*
 * Minimum numbers of pmonrs that must go to Active state per second in order
 * to consider rotation to be effective.
 */
#define CQM_DEFAULT_MIN_PROGRESS_RATE 1
static unsigned int __cqm_min_progress_rate = CQM_DEFAULT_MIN_PROGRESS_RATE;

/*
 * If we fail to assign any RMID for intel_cqm_rotation because cachelines are
 * still tagged with RMIDs in limbo even after having stolen enough rmids (a
 * maximum number of rmids in limbo at any time), then we increment the dirty
 * threshold to allow at least one RMID to be recycled. This mitigates the
 * problem caused when cachelines tagged with a RMID are not evicted but
 * it introduces error in the occupancy reads but allows the rotation of rmids
 * to proceed.
 *
 * __intel_cqm_max_threshold provides an upper bound on the threshold,
 * and is measured in bytes because it's exposed to userland.
 * It's units are bytes must be scaled by cqm_l3_scale to obtain cache lines.
 */
static unsigned int __intel_cqm_max_threshold;

#ifdef CONFIG_CGROUP_PERF

struct cgrp_cqm_info {
	/* Should the cgroup be continuously monitored? */
	bool		cont_monitoring;
	struct monr	*monr;
};

# define css_to_perf_cgroup(css_) container_of(css_, struct perf_cgroup, css)
# define cgrp_to_cqm_info(cgrp_) ((struct cgrp_cqm_info *)cgrp_->arch_info)
# define css_to_cqm_info(css_) cgrp_to_cqm_info(css_to_perf_cgroup(css_))

#endif
