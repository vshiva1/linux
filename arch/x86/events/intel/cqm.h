/*
 * Intel Cache Quality-of-Service Monitoring (CQM) support.
 *
 * A Resource Manager ID (RMID) is a u32 value that, when programmed in a
 * logical CPU, will allow the LLC cache to associate the changes in occupancy
 * generated by that cpu (cache lines allocations - deallocations) to the RMID.
 * If a RMID has been assigned to a thread T long enough for all cache lines
 * used by T to be allocated, then the occupancy reported by the hardware
 * equals the total cache occupancy for T.
 *
 * Groups of threads that are to be monitored together (such as cgroups
 * or processes) can shared a RMID.
 *
 * This driver implements a tree hierarchy of Monitored Resources (monr). Each
 * monr is a cgroup, a process or a thread that needs one single RMID.
 *
 * Since the number of RMIDs is relatively small to the number of potential
 * monitored elements, RMIDs must be "rotated" among all monitored elements.
 */

#include <linux/perf_event.h>
#include <asm/pqr_common.h>
#include <asm/topology.h>

/*
 * struct prmid: Package RMID. Per-package wrapper for a rmid.
 * @last_read_value:	Least read value.
 * @last_read_time:	Time last read, used when throtling read rate.
 * @pool_entry:		Attaches to a prmid pool in cqm_pkg_data.
 * @rmid:		The rmid value to be programed in hardware.
 *
 * Its accesors ensure that CQM events for this rmid are read atomically and
 * allow to throtle the frequency of reads to up to one each
 * __rmid_min_update_time ms.
 */
struct prmid {
	atomic64_t		last_read_value;
	atomic64_t		last_read_time;
	struct list_head	pool_entry;
	u32			rmid;
};

/*
 * struct pkg_data: Per-package CQM data.
 * @max_rmid:			Max rmid valid for cpus in this package.
 * @prmids_by_rmid:		Utility mapping between rmid values and prmids.
 *				XXX: Make it an array of prmids.
 * @free_prmid_pool:		Free prmids.
 * @pkg_data_mutex:		Hold for stability when modifying pmonrs
 *				hierarchy.
 * @pkg_data_lock:		Hold to protect variables that may be accessed
 *				during process scheduling. The locks for all
 *				packages must be held when modifying the monr
 *				hierarchy.
 * @rotation_cpu:               CPU to run @rotation_work on, it must be in the
 *                              package associated to this instance of pkg_data.
 */
struct pkg_data {
	u32			max_rmid;
	/* Quick map from rmids to prmids. */
	struct prmid		**prmids_by_rmid;

	/*
	 * Pools of prmids used in rotation logic.
	 */
	struct list_head	free_prmids_pool;

	struct mutex		pkg_data_mutex;
	raw_spinlock_t		pkg_data_lock;

	int			rotation_cpu;
};

extern struct pkg_data **cqm_pkgs_data;

static inline u16 __cqm_pkgs_data_next_online(u16 pkg_id)
{
	while (!cqm_pkgs_data[++pkg_id] && pkg_id < topology_max_packages())
		;
	return pkg_id;
}

static inline u16 __cqm_pkgs_data_first_online(void)
{
	if (cqm_pkgs_data[0])
		return 0;
	return __cqm_pkgs_data_next_online(0);
}

/* Iterate for each online pkgs data */
#define cqm_pkg_id_for_each_online(pkg_id__) \
	for (pkg_id__ = __cqm_pkgs_data_first_online(); \
	     pkg_id__ < topology_max_packages(); \
	     pkg_id__ = __cqm_pkgs_data_next_online(pkg_id__))

#define __pkg_data(pmonr, member) cqm_pkgs_data[pmonr->pkg_id]->member

/*
 * Utility function and macros to manage per-package locks.
 * Use macros to keep flags in caller's stace.
 * Hold lock in all the packages, required to alter the monr hierarchy
 */
static inline void monr_hrchy_acquire_mutexes(void)
{
	int i;

	cqm_pkg_id_for_each_online(i)
		mutex_lock_nested(&cqm_pkgs_data[i]->pkg_data_mutex, i);
}

# define monr_hrchy_acquire_raw_spin_locks_irq_save(flags, i) \
	do { \
		raw_local_irq_save(flags); \
		cqm_pkg_id_for_each_online(i) {\
			raw_spin_lock_nested( \
				&cqm_pkgs_data[i]->pkg_data_lock, i); \
		} \
	} while (0)

#define monr_hrchy_acquire_locks(flags, i) \
	do {\
		monr_hrchy_acquire_mutexes(); \
		monr_hrchy_acquire_raw_spin_locks_irq_save(flags, i); \
	} while (0)

static inline void monr_hrchy_release_mutexes(void)
{
	int i;

	cqm_pkg_id_for_each_online(i)
		mutex_unlock(&cqm_pkgs_data[i]->pkg_data_mutex);
}

# define monr_hrchy_release_raw_spin_locks_irq_restore(flags, i) \
	do { \
		cqm_pkg_id_for_each_online(i) {\
			raw_spin_unlock(&cqm_pkgs_data[i]->pkg_data_lock); \
		} \
		raw_local_irq_restore(flags); \
	} while (0)

#define monr_hrchy_release_locks(flags, i) \
	do {\
		monr_hrchy_release_raw_spin_locks_irq_restore(flags, i); \
		monr_hrchy_release_mutexes(); \
	} while (0)

static inline void monr_hrchy_assert_held_mutexes(void)
{
	int i;

	cqm_pkg_id_for_each_online(i)
		lockdep_assert_held(&cqm_pkgs_data[i]->pkg_data_mutex);
}

static inline void monr_hrchy_assert_held_raw_spin_locks(void)
{
	int i;

	cqm_pkg_id_for_each_online(i)
		lockdep_assert_held(&cqm_pkgs_data[i]->pkg_data_lock);
}
#ifdef CONFIG_LOCKDEP
static inline int monr_hrchy_count_held_raw_spin_locks(void)
{
	int i, nr_held = 0;

	cqm_pkg_id_for_each_online(i) {
		if (lockdep_is_held(&cqm_pkgs_data[i]->pkg_data_lock))
			nr_held++;
	}
	return nr_held;
}
#endif

/*
 * Time between execution of rotation logic. The frequency of execution does
 * not affect the rate at which RMIDs are recycled, except by the delay by the
 * delay updating the prmid's and their pools.
 * The rotation period is stored in pmu->hrtimer_interval_ms.
 */
#define CQM_DEFAULT_ROTATION_PERIOD 1200	/* ms */

/*
 * __intel_cqm_max_threshold provides an upper bound on the threshold,
 * and is measured in bytes because it's exposed to userland.
 * It's units are bytes must be scaled by cqm_l3_scale to obtain cache lines.
 */
static unsigned int __intel_cqm_max_threshold;
